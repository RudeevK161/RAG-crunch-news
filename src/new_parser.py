import requests
import json
import hashlib
from datetime import datetime, timedelta, date
import time
import random
from bs4 import BeautifulSoup
import re
import uuid


def parse_techcrunch_pagination(target_articles, max_pages):
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å–∏ —Å –ø–∞–≥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü TechCrunch AI"""

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://www.google.com/',
    }

    session = requests.Session()
    session.headers.update(headers)

    all_articles = {}
    page = 1

    six_months_ago = datetime.now() - timedelta(days=180)

    print(f"–ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ TechCrunch AI...")
    print(f"–¶–µ–ª—å: {target_articles} —Å—Ç–∞—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 6 –º–µ—Å—è—Ü–µ–≤")
    print("=" * 60)

    while len(all_articles) < target_articles and page <= max_pages:
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if page == 1:
            url = "https://techcrunch.com/category/artificial-intelligence/"
        else:
            url = f"https://techcrunch.com/category/artificial-intelligence/page/{page}/"

        print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {url}")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Å–ª—É—á–∞–π–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π
            time.sleep(random.uniform(2, 4))

            response = session.get(url, timeout=15)

            if response.status_code == 404:
                print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (404). –ü—Ä–µ–∫—Ä–∞—â–∞—é –ø–∞—Ä—Å–∏–Ω–≥.")
                break

            if response.status_code != 200:
                print(f"  –û—à–∏–±–∫–∞ {response.status_code}. –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
                page += 1
                continue

            soup = BeautifulSoup(response.content, 'html.parser')

            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –°–ï–õ–ï–ö–¢–û–†: –ù–∞—Ö–æ–¥–∏–º –í–°–ï —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã TechCrunch
            article_elements = soup.find_all('li', class_='wp-block-post')

            if not article_elements:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                article_elements = soup.select('article.post-block, .post-block')

            if not article_elements:
                print(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞—Ç—å–∏. –ü—Ä–æ–±—É—é –¥—Ä—É–≥–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä...")
                # –ï—â–µ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ–∏—Å–∫–∞
                article_elements = soup.select('[class*="post-"]')

            if not article_elements or len(article_elements) == 0:
                print(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} —Å—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—è—é HTML...")
                # –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                # print(soup.prettify()[:2000])  # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                break

            print(f"  –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(article_elements)}")

            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            articles_on_page = 0
            published_date = None

            for article_element in article_elements:
                try:
                    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –°–ï–õ–ï–ö–¢–û–†
                    link_element = article_element.select_one('a.loop-card__title-link')

                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Å, –∏—â–µ–º –ø–æ –∞—Ç—Ä–∏–±—É—Ç—É data-destinationlink
                    if not link_element:
                        link_element = article_element.select_one('a[data-destinationlink]')

                    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ª—é–±—É—é —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏
                    if not link_element:
                        link_element = article_element.select_one('a[href*="/202"]')

                    if not link_element:
                        print(f"    –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –≤ —Å—Ç–∞—Ç—å–µ")
                        continue

                    article_url = link_element.get('href', '')
                    if not article_url:
                        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ data-destinationlink
                        article_url = link_element.get('data-destinationlink', '')

                    if not article_url:
                        continue

                    if not article_url.startswith('http'):
                        article_url = 'https://techcrunch.com' + article_url

                    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –°–ï–õ–ï–ö–¢–û–†
                    title_element = article_element.select_one('.loop-card__title')
                    if not title_element:
                        title_element = link_element

                    article_title = title_element.get_text(strip=True) if title_element else ""
                    """
                    
                    # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –°–ï–õ–ï–ö–¢–û–†
                    date_element = article_element.find('time', recursive=True)

                    print(f" Date {date_element}")
                    if date_element:
                        datetime_str = date_element.get('datetime')

                        if datetime_str:
                            date_str = datetime_str[:10]

                            try:
                                published_date = date.fromisoformat(date_str)
                                six_months_ago_date = six_months_ago.date()

                                if published_date < six_months_ago_date:
                                    print(f"    –°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è: {published_date}")
                                    continue
                            except Exception as e:
                                print(f"    –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∞—Ç: {e}")
                    else:
                        published_date = date.today()
                    """

                    date_from_url = article_url.split('/')[3:6]
                    if date_from_url[0][:2]=='20':
                        published_date = '-'.join(date_from_url)


                    # 5. –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ (–ø–æ–∑–∂–µ)
                    print(f"    –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {article_title[:70]}...")

                    # 7. –°–æ–∑–¥–∞–µ–º ID
                    article_id = str(uuid.uuid5(uuid.NAMESPACE_URL, article_url))
                    #article_id = len(all_articles) + 1

                    # 8. –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—å—é (–ø–æ–∫–∞ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞)
                    if article_url not in all_articles:
                        all_articles[article_url] = {
                            "id": article_id,
                            "title": article_title,
                            "url": article_url,
                            "published_time": published_date,
                        }

                        articles_on_page += 1
                        print(f"    ‚úì –î–æ–±–∞–≤–ª–µ–Ω–∞: {article_title[:60]}...")

                except Exception as e:
                    print(f"    –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue

            print(f"  –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –¥–æ–±–∞–≤–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {articles_on_page}")
            print(f"  –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_articles)}/{target_articles}")
            print(f"  –ü—Ä–æ–≥—Ä–µ—Å—Å: {len(all_articles) / target_articles * 100:.1f}%")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –°–ï–õ–ï–ö–¢–û–†
            next_page_link = soup.select_one('.wp-block-query-pagination-next, a[rel="next"]')
            if not next_page_link and len(article_elements) < 10:
                print("  –ù–µ—Ç —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –º–∞–ª–æ —Å—Ç–∞—Ç–µ–π. –ü—Ä–µ–∫—Ä–∞—â–∞—é.")
                break

            page += 1

            print()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏

        except requests.exceptions.Timeout:
            print(f"  –¢–∞–π–º–∞—É—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
            page += 1
            continue
        except Exception as e:
            print(f"  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
            break

    # üîß –î–û–ë–ê–í–õ–Ø–ï–ú –ü–û–õ–£–ß–ï–ù–ò–ï –ü–û–õ–ù–û–ì–û –¢–ï–ö–°–¢–ê –î–õ–Ø –í–°–ï–• –°–û–ë–†–ê–ù–ù–´–• –°–¢–ê–¢–ï–ô
    print(f"\n–ù–∞—á–∏–Ω–∞—é –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è {len(all_articles)} —Å—Ç–∞—Ç–µ–π...")

    articles_with_text = []
    processed_count = 0

    for i, (article_url, article_data) in enumerate(list(all_articles.items()), 1):
        try:
            total_articles = len(all_articles)
            print(f"[{i}/{total_articles}] –ü–æ–ª—É—á–∞—é —Ç–µ–∫—Å—Ç: {article_data['title'][:60]}...")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
            full_text = extract_full_article_text(session, article_url)

            if not full_text or len(full_text) < 500:
                print(f"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ç—å—é –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
                del all_articles[article_url]
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ AI —Å—Ç–∞—Ç—å—è
           # if not is_ai_article(article_data['title'], full_text):
           #     print(f"–ü—Ä–æ–ø—É—Å–∫–∞—é (–Ω–µ AI —Ç–µ–º–∞)")
           #     # –£–¥–∞–ª—è–µ–º –Ω–µ-AI —Å—Ç–∞—Ç—å—é
           #     del all_articles[article_url]
           #     continue

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –∫ –¥–∞–Ω–Ω—ã–º —Å—Ç–∞—Ç—å–∏
            article_data["text"] = full_text
            article_data["word_count"] = len(full_text.split())

            articles_with_text.append(article_data)
            processed_count += 1

            print(f"    ‚úì –¢–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω: {article_data['word_count']} —Å–ª–æ–≤")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —Å—Ç–∞—Ç–µ–π
            time.sleep(random.uniform(0.5, 1.5))

        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é —Å—Ç–∞—Ç—å—é
            if article_url in all_articles:
                del all_articles[article_url]
            continue

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ —Å–ø–∏—Å–æ–∫ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    articles_list = list(all_articles.values())
    #articles_list.sort(key=lambda x: x['published_time'], reverse=True)

    print("\n" + "=" * 60)
    print(f"–ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
    print(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(articles_list)}")
    print(f"–°—Ç–∞—Ç–µ–π —Å —Ç–µ–∫—Å—Ç–æ–º: {len(articles_with_text)}")
    print("=" * 60)

    return articles_list[:target_articles]


def extract_full_article_text(session, url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"""
    try:
        time.sleep(random.uniform(0.5, 0.8))  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        response = session.get(url, timeout=15)

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.content, 'html.parser')

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup.select('script, style, iframe, nav, footer, .advertisement, .share-buttons, .comments'):
            element.decompose()

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_selectors = [
            '.article-content',
            '.entry-content',
            '.single-post-content',
            'article .content',
            '.article__content',
            '.article-body',
            '.post-content',
            'article > div',
            '[class*="content"]',
            '.rich-text'
        ]


        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                text_elements = content_div.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                text_parts = []

                for element in text_elements:
                    text = element.get_text(strip=True)
                    if text and len(text) > 30:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                        text_parts.append(text)

                if text_parts:
                    full_text = '\n\n'.join(text_parts)
                    if len(full_text) > 300:
                        return full_text

        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥: –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
        article_tag = soup.find('article')
        if article_tag:
            paragraphs = article_tag.find_all('p')
            if paragraphs:
                text_parts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50]
                if text_parts:
                    full_text = '\n\n'.join(text_parts)
                    if len(full_text) > 300:
                        return full_text

        return None

    except Exception as e:
        print(f"      –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        return None


def is_ai_article(title, text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –∫ AI —Ç–µ–º–∞—Ç–∏–∫–µ"""
    content = (title + ' ' + text).lower()

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ AI
    ai_keywords = [
        'ai', 'artificial intelligence', 'machine learning', 'deep learning',
        'neural network', 'llm', 'gpt', 'chatgpt', 'generative ai',
        'computer vision', 'nlp', 'natural language', 'transformer',
        'openai', 'anthropic', 'midjourney', 'stable diffusion',
        'large language model', 'prompt engineering', 'diffusion model',
        'reinforcement learning', 'autonomous', 'robotics', 'algorithm'
    ]

    # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    matches = sum(1 for keyword in ai_keywords if keyword in content)

    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    patterns = [
        r'\bAI\b', r'\bA\.I\.', r'machine learning', r'deep learning',
        r'generative (ai|model)', r'neural (network|net)'
    ]

    pattern_matches = sum(len(re.findall(pattern, content, re.IGNORECASE))
                          for pattern in patterns)

    # –°—Ç–∞—Ç—å—è —Å—á–∏—Ç–∞–µ—Ç—Å—è AI, –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    return (matches >= 3) or (pattern_matches >= 2)


def save_articles_to_json(articles, filename=None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ JSON —Ñ–∞–π–ª"""
    if not filename:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        filename = f"techcrunch_ai_{len(articles)}_articles_{timestamp}.json"

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è JSON
    for article in articles:
        if 'published_time' in article and isinstance(article['published_time'], datetime):
            article['published_time'] = article['published_time'].isoformat()

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    print(f"–°—Ç–∞—Ç—å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
    return filename


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 60)
    print("TECHCRUNCH AI ARTICLE PARSER")
    print("–ü–∞—Ä—Å–∏–Ω–≥ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ /category/artificial-intelligence/page/N/")
    print("=" * 60)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    TARGET_ARTICLES = 7000
    MAX_PAGES = 185  # 50 —Å—Ç—Ä–∞–Ω–∏—Ü √ó ~20 —Å—Ç–∞—Ç–µ–π = ~1000 —Å—Ç–∞—Ç–µ–π

    start_time = time.time()

    # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å–∏
    articles = parse_techcrunch_pagination(TARGET_ARTICLES, MAX_PAGES)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if articles:
        filename = save_articles_to_json(articles)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        elapsed_time = time.time() - start_time
        avg_words = sum(a['word_count'] for a in articles) / len(articles)

        print(f"\n{'=' * 60}")
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"–°–æ–±—Ä–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time / 60:.1f} –º–∏–Ω—É—Ç")
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å—Ç–∞—Ç—å–∏: {avg_words:.0f} —Å–ª–æ–≤")

        if articles:
            dates = [a['date'] for a in articles if 'date' in a]
            if dates:
                print(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {min(dates)} - {max(dates)}")

        print(f"\n–ü–µ—Ä–≤—ã–µ 3 —Å—Ç–∞—Ç—å–∏:")
        for i, article in enumerate(articles[:3]):
            print(f"{i + 1}. {article['title'][:80]}...")
            print(f"   –î–∞—Ç–∞: {article.get('date', 'N/A')}, –°–ª–æ–≤: {article['word_count']}")

        print(f"\n–§–∞–π–ª: {filename}")
    else:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç—å–∏")


if __name__ == "__main__":
    main()